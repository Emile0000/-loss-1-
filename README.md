# tensorflow.keras.loss.categorical_crossentropy
現在要使用LSTM模型進行五元分類：[1, 2, 3, 4, 5] <br>
使用activation function = softmax, loss function = categorical_crossentropy的組合 <br>

# 公式
\[ loss = - \frac{1}{batch\_size} \sum_{i}^{batch\_size} y_{true} \ln(y_{pred}) \] <br>
batch_size是這個epoch中有幾筆資料，y_{true}是真實值，y_pred是預測值 <br>

# 範例
[1, 2, 3, 4, 5]在經過one-hot encoding之後： <br>
[[1, 0, 0, 0, 0], <br>
 [0, 1, 0, 0, 0], <br>
 [0, 0, 1, 0, 0], <br>
 [0, 0, 0, 1, 0], <br>
 [0, 0, 0, 0, 1]] <br>

整個資料集為： <br>
[3, 5, 2, 2, 3, 4] <br>
經過one-hot encding後： <br>
[[0., 0., 1., 0., 0.], <br>
[0., 0., 0., 0., 1.], <br>
[0., 1., 0., 0., 0.], <br>
[0., 1., 0., 0., 0.], <br>
[0., 0., 1., 0., 0.], <br>
[0., 0., 0., 1., 0.]]$ <br>
在這個epoch結束時，預測值為： <br>
[[0.6268896, 0.25881532, 0.09150913, 0.02037693, 0.00240903], <br>
[0.09165806, 0.3015705, 0.2891071, 0.26664162, 0.05102272], <br>
[0.08631954, 0.11195231, 0.28462902, 0.30316573, 0.21393338], <br>
[0.11869003, 0.18319334, 0.18643036, 0.3534843, 0.15820198], <br>
[0.10678157, 0.18504941, 0.25989717, 0.26833403, 0.17993775], <br>
[0.11031441, 0.15937562, 0.18078934, 0.4133129, 0.13620767]] <br>

則每一筆資料都會有對應的loss： <br>
loss 1 = - (0 * ln(0.6268896) + 0 * ln(0.25881532) + 1 * ln(0.09150913) + 0 * ln(0.02037693) + 0 * ln(0.00240903)) = -ln(0.09150913) = 2.39131654，真實值3對應的機率是0.09150913 <br>
loss 2 = - (0 * ln(0.09165806) + 0 * ln(0.3015705) + 0 * ln(0.2891071) + 0 * ln(0.26664162) + 1 * ln(0.05102272)) = -ln(0.05102272) = 2.97548426，真實值5對應的機率是0.05102272 <br>
loss 3 = ... = 2.18968228，真實值2對應的機率是0.11195231 <br>
loss 4 = ... = 1.69721319，真實值2對應的機率是0.18319334 <br>
loss 5 = ... = 1.34746916，真實值3對應的機率是0.25989717 <br>
loss 6 = ... = 0.88355029，真實值4對應的機率是0.4133129 <br>

而這個epoch中的loss會是(2.39131654 + 2.97548426 + 2.18968228 + 1.69721319 + 1.34746916 + 0.88355029) / 6 = 1.9141192850880648 <br>

# loss = 1的情況 <br>
因為使用自然對數，而loss = -ln(e^(-1)) = 1表示這五類的真實值對應的機率都是e^(-1)，約等於0.36 <br>

# loss > 1的情況 <br>
所以根據上面loss = 1的情況判斷，當這6筆（所有筆數）真實值對應的機率最大都是e^(-1)，並且至少其中一筆的機率 < e^(-1)，loss就會 > 1 <br>
例如： <br>
probilities = [e^(-1) - 0.0001, e^(-1), e^(-1), e^(-1), e^(-1), e^(-1)] <br>
losses = [1.0002718651348228, 1, 1, 1, 1, 1] <br>
-> final loss = (1.0002718651348228 + 1.0 + 1.0 + 1.0 + 1.0 + 1.0) / 6 = 1.0002718651348228 <br>
